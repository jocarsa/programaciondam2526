cuando trabajamos con redes neuronales cada una de las neuronas tiene un peso diferente cada una de las neuronas tiene un pequeño pesito que va entre 0 y 1 generalmente va en trazado que uno puede ir en otros sistemas pero bueno es una convención así que cada neurona tiene un peso diferente dependiendo un poco de lo que esté haciendo en ese momento 0 4 esa tiene 0 7 sacada en una puede tener un peso diferente dependiendo de la función que esté realizando en este vídeo voy a explicar un concepto que es el concepto de bach por paguéis jon la propagación hacia atrás es otro de los conceptos de deep learning es otro de los conceptos de aprendizaje profundo y realmente es un concepto bastante similar a cómo funciona la mente humana cuando realizamos un primer cálculo de inteligencia artificial distribuida en varias capas en el momento en el que empezamos a trabajar con estímulos a cada neurona le acabamos dando un peso específico es decir cuando cada neurona pasa la información a la siguiente capa lo que ocurre en este caso es que la neurona multiplica el peso de la información que le ha entrado para decir cómo de importante o como de poco importante es con respecto al resto de neuronas la primera vez que ejecutamos un cálculo neuronal lo que ocurre es que las neuronas adquieren un cierto peso en base al éxito o al fracaso que ha tenido la operación que representan de esa forma al final tenemos como ya digo voy a duplicar por aquí números simplemente con una cuestión representativa arriba y lo mismo podría hacer con el resto lo mismo podría hacer con el resto de números de neuronas pero hay veces que tras un primer reparto de pesos cuando llegamos al resultado final podemos ver que en el primer reparto de pesos el rasgo de exitoso pero cuando cambiamos el estímulo de entrada puede que los pesos no sean totalmente convenientes sean mejorables sean afín hables todo es mejorable al final y cuando hacemos un primer cálculo neuronal en base a un primer estímulo puede que los pesos que hayamos encontrado sean beneficiosos para ese primer estímulo pero no sean los más aptos para cualquier tipo de estímulo de entrada lo que hace la propagación es que cuando cambiamos el estímulo y vemos que el resultado de salida no es tan bueno como el que esperábamos la red neuronal es capaz de volver atrás es capaz de recalcular los pesos es capaz de ajustar los pesos y es capaz de entonces de volver a calcular hacia adelante hasta que el resultado sea más óptimo básicamente la propagación hacia atrás la abacc propagación lo que hace es en un momento dado tomar una serie de conclusiones pero más adelante lo que hace es asumir que puede que esas conclusiones no sean las mejores y lo que hace es un poco cambiar su mente lo que hace es volver a recalcular asumir que lo que era va por cierto igual no es totalmente cierto y rehacerse mentalmente recalcular los pesos y buscar el funcionamiento óptimo del sistema el de planning es uno de los sistemas de aprendizaje autónomo que tienen los sistemas de inteligencia artificial y básicamente lo que hace es no depender de una persona humana que vea el reto del cálculo y diga esto no está bien voy a reajustar los pesos porque insisto cada capa neuronal en este caso tenemos seis neuronas porque esto no es más que un esquema pero una capa puede tener decenas de miles de neuronas y que por tanto que las ajuste un ser humano a veces puede no ser factible directamente sino que lo que hace el sistema es real darse a sí mismo lo que hace el sistema es tirar para atrás cambiar los pesos y probar y puede que el cambio sea peor en lugar de será mejor pero por eso un sistema puede repetir miles de veces millones de veces este reajuste por segundo hasta que en unos pocos segundos encuentra el ajuste que más le conviene la idea con el back pro para gates son es que un sistema neuronal de múltiples capas es algo más complejo de lo que en un primer momento puede parecer pero dentro de que es más complejo es un sistema más inteligente desde el punto de vista en el que tiene cierta autonomía para buscar cuál es su funcionamiento ideal